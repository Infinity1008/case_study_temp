{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding useful data in the wild: Perspectives from data journalism \n",
    "\n",
    "[This](https://www.reddit.com/r/dataisbeautiful/comments/t9a764/oc_equipment_losses_in_the_ukrainerussian_war/) reddit post is an interesting read. This is a submission to `r/dataisbeautiful`, a reddit sub that I am subscribed to. It tries to compare the military losses on both sides of the Russian-Ukrainian conflict. A cursory look at the charts presented here, gives one, the impression that numbers are the absolute truth! Dig a little deep and when you see the source cited [here](https://www.oryxspioenkop.com/2022/02/attack-on-europe-documenting-equipment.html?m=1), does one realise that:\n",
    "1. Data is extremely dirty in the wild.\n",
    "2. The biases and inaccuracies don't become apparent until one digs into the source.\n",
    "\n",
    "\n",
    "### 1. Getting the relevant information from data\n",
    "\n",
    "Look at the exhibit below and you will see that the relevant data is not in a form you can do any analysis on\n",
    "\n",
    "![](osint_main.png)\n",
    "\n",
    "How does one even start putting this data into a reasonable structure?\n",
    "\n",
    "One reasonable schema is the following\n",
    "\n",
    "| Category | Total | Damaged | Destroyed | Captured | Abandoned |\n",
    "| :------- | :---- | :------ | :-------- | :------- | :-------- |\n",
    "| Armour   | 120   | 100     | 10        | 8        | 2         |\n",
    "| ..       | ..    | ..      | ..        | ..       | ..        |\n",
    "| ..       | ..    | ..      | ..        | ..       | ..        |\n",
    "\n",
    "But how does one go about creating such a schema? For starters one can scrape this data and put it in a tabular form\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "url = 'https://www.oryxspioenkop.com/2022/02/attack-on-europe-documenting-equipment.html?m=1'\n",
    "raw_html = requests.get(url).text\n",
    "soup = BeautifulSoup(raw_html,'html.parser')\n",
    "main_categories = soup.find('div',attrs={'itemprop':'articleBody'}).find_all(\"h3\")\n",
    "cleaned_cats = []\n",
    "for cat in main_categories:\n",
    "    if cat.text.strip()!='':\n",
    "        cleaned_cats.append(cat.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_key = re.compile(r'[a-zA-Z]+')\n",
    "pattern_value = re.compile(r'\\d+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_dict = []\n",
    "for cat in cleaned_cats:\n",
    "    if \"Trucks, Vehicles and Jeeps\" in cat:\n",
    "        cat = cat.replace(\"Trucks, Vehicles and Jeeps\",\"Trucks Vehicles and Jeeps\")\n",
    "    cat = cat.replace(\"of which\",\"\")\n",
    "    parts = cat.split(\",\")\n",
    "    parts = [p.strip() for p in parts]\n",
    "    keys = []\n",
    "    values = []\n",
    "    details = {}\n",
    "    for idx, p in enumerate(parts):\n",
    "        key = re.findall(pattern_key,p)\n",
    "        key = \" \".join(key)\n",
    "        value = re.findall(pattern_value,p)[0]\n",
    "        keys.append(key)\n",
    "        values.append(value)\n",
    "        if idx!=0:\n",
    "            details[key]=value\n",
    "    cnt_dict.append({keys[0]:values[0],'details':details})\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = []\n",
    "destroyed = []\n",
    "damaged = []\n",
    "abandoned = []\n",
    "captured = []\n",
    "total = []\n",
    "\n",
    "for counts in cnt_dict:\n",
    "    for key in counts:\n",
    "        if key!='details':\n",
    "            category.append(key)\n",
    "            total.append(int(counts[key]))\n",
    "        else:\n",
    "            destroyed.append(int(counts['details'].get('destroyed',0)))\n",
    "            damaged.append(int(counts['details'].get('damaged',0)))\n",
    "            abandoned.append(int(counts['details'].get('abandoned',0)))\n",
    "            captured.append(int(counts['details'].get('captured',0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "result['Category'] = category\n",
    "result['Total'] = total\n",
    "result['Destroyed'] = destroyed\n",
    "result['Damaged'] = damaged\n",
    "result['Abandoned'] = abandoned\n",
    "result['Captured'] = captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "russia = result.iloc[1:23]\n",
    "ukraine = result.iloc[25:]\n",
    "russia.to_csv(\"../../data/data_acquisition/russian_losses_aggregates.csv\",index=False)\n",
    "ukraine.to_csv(\"../../data/data_acquisition/ukrainian_losses_aggregates.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category,Total,Destroyed,Damaged,Abandoned,Captured\n",
      "Tanks,156,53,2,30,71\n",
      "Armoured Fighting Vehicles,98,33,0,18,45\n",
      "Infantry Fighting Vehicles,141,55,0,25,61\n",
      "Armoured Personnel Carriers,55,16,0,10,29\n"
     ]
    }
   ],
   "source": [
    "!head -5 ../../data/data_acquisition/russian_losses_aggregates.csv ## windows users can use a different command here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this jugglery with the code to be just able to fetch the relevant data. One can go a step further and get a detailed break-up of numbers by each category. That will be left as an exercise and solution will be shared later.\n",
    "\n",
    "What next?\n",
    "\n",
    "We can look at two specific areas\n",
    "- How do we share this data once it has been scrapped?\n",
    "- What are some of the biases we need to inform the users of this data?\n",
    "\n",
    "\n",
    "**1. Sharing Data**\n",
    "\n",
    "There are a couple of ways to go about sharing data:\n",
    "1. Use a platform such as [kaggle](https://www.kaggle.com/datasets), to host your datasets. Do provide an appropriate license.\n",
    "2. Use github to store not only the dataset but also the source code.\n",
    "3. Create an api service, using which one can consume the dataset.\n",
    "\n",
    "**2. Biases in the collected data**\n",
    "\n",
    "Most data collection exercises will have some bias built into it. For example, in the current case the following biases are present:\n",
    "1. The data has been collected by the volunteers sympathetic towards the Ukrainian cause. There could be under-reporting of Ukrainian losses and over-reporting of Russian losses.\n",
    "2. The relative size of Russian and Ukrainian militaries is missing, so there is little point in comparing the absolute number of losses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7daf05c69c155aebd2e559915f14874f994643a3478331c2f87e5ba1291b428"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
